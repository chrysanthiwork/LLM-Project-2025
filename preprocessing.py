import os
import re
import unicodedata
import requests
import snowballstemmer
import enchant
import nltk
from nltk.corpus import wordnet

nltk.download('wordnet')

# Î•Î³ÎºÎ±Ï„Î¬ÏƒÏ„Î±ÏƒÎ· Î»ÎµÎ¾Î¹ÎºÎ¿Ï Î±Î³Î³Î»Î¹ÎºÎ®Ï‚
english_dict = enchant.Dict("en_US")
stemmer = snowballstemmer.stemmer('english')

# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î³Î¹Î± Î±Ï†Î±Î¯ÏÎµÏƒÎ· Ï„ÏŒÎ½Ï‰Î½
def remove_accents(text):
    return ''.join(
        char for char in unicodedata.normalize('NFD', text)
        if unicodedata.category(char) != 'Mn'
    )

# Î£Ï…Î½Î¬ÏÏ„Î·ÏƒÎ· Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ stopwords Î±Ï€ÏŒ URL
def read_stopwords_from_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        stopwords = response.text.splitlines()
        return [word.strip() for word in stopwords]
    else:
        print(f"Failed to retrieve the file. Status code: {response.status_code}")
        return []

# Î‘Ï†Î±Î¯ÏÎµÏƒÎ· stopwords Î±Ï€ÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
def remove_stopwords(text, stopwords_list):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stopwords_list]
    return ' '.join(filtered_words)

# ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ ÎµÎ³ÎºÏ…ÏÏŒÏ„Î·Ï„Î±Ï‚ Î»Î­Î¾Î·Ï‚ (Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÎµ Î»ÎµÎ¾Î¹ÎºÏŒ Î® WordNet)
def is_valid_word(word):
    word = re.sub(r'\W+', '', word).strip().lower()
    return english_dict.check(word) or bool(wordnet.synsets(word))


# ============ ÎšÏÏÎ¹Î± Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ============ #

root_folder = 'downloaded_articles'

documents = {}
texts = []
vocabulary = set()

# Î”Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ ÏŒÎ»Î± Ï„Î± Î±ÏÏ‡ÎµÎ¯Î±
for folder_name in os.listdir(root_folder):
    folder_path = os.path.join(root_folder, folder_name)
    if os.path.isdir(folder_path):
        articles = []
        for file_name in os.listdir(folder_path):
            if file_name.endswith('.txt'):
                file_path = os.path.join(folder_path, file_name)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    articles.append(content)
        documents[folder_name] = articles

# Î”Î¹Î±Î²Î¬Î¶Î¿Ï…Î¼Îµ stopwords
greek_stopwords = read_stopwords_from_url(
    'https://raw.githubusercontent.com/stopwords-iso/stopwords-el/refs/heads/master/stopwords-el.txt'
)
english_stopwords = read_stopwords_from_url(
    'https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.txt'
)

# Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½
texts = []
texts_untouched = []
vocabulary = set()
doc_labels = []  # ğŸ‘ˆ ÎºÏÎ±Ï„Î¬ÎµÎ¹ Ï„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î± Ï„Î¿Ï… ÎºÎ¬Î¸Îµ text

for category, articles in documents.items():  # ğŸ‘ˆ Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ ÎºÎ±Î¹ Ï„Î¿ category
    for article in articles:
        article = article.lower()
        article = re.sub(r'[^a-zA-ZÎ‘-Î©Î±-Ï‰Î†ÎˆÎ‰ÎŠÎŒÎÎÎ¬Î­Î®Î¯ÏŒÏÏ\s]', '', article)
        article = remove_accents(article)
        article = remove_stopwords(article, english_stopwords)
        texts_untouched.append(article)
        words = article.split()

        processed_words = []
        for word in words:
            word = re.sub(r'\W+', '', word).strip()
            if is_valid_word(word):
                stemmed = stemmer.stemWords([word])[0]
                processed_words.append(stemmed)
                vocabulary.add(stemmed)
            else:
                print(f"Rejected: {word}")

        cleaned_text = ' '.join(processed_words)
        texts.append(cleaned_text)
        doc_labels.append(category)  # ğŸ‘ˆ Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎµ Ï„Î·Î½ ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±

print(texts_untouched)
# Î ÏÎ¿Î²Î¿Î»Î® Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
'''print("\n--- ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î­Î½Î± ÎšÎµÎ¯Î¼ÎµÎ½Î± ---")
print(texts)

print("\n--- ÎœÎ­Î³ÎµÎ¸Î¿Ï‚ Î›ÎµÎ¾Î¹Î»Î¿Î³Î¯Î¿Ï… ---")
print(vocabulary)'''

__all__ = ['texts', 'documents', 'doc_labels']
