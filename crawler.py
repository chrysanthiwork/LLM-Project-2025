import os
import re
import requests
from bs4 import BeautifulSoup
import pandas as pd

# === LOAD JSON ===
df = pd.read_json("cluster_sample.json")

# === CLEAN URLs ===
def clean_urls(nested_list):
    flat_list = [item for sublist in nested_list for item in sublist]
    return [url.replace("\\/", "/") for url in flat_list]

grouped_dict = df.groupby('label')['urls'].apply(lambda nested: clean_urls(nested)).to_dict()

# === CRAWLER FUNCTIONS ===

def sanitize_filename(s):
    return re.sub(r'[^a-zA-Z0-9_\-]', '_', s)

def extract_text_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Î‘Ï†Î±Î¯ÏÎµÏƒÎ· scripts, styles, meta, nav, footer ÎºÎ»Ï€
        for tag in soup(["script", "style", "meta", "nav", "footer", "noscript", "aside", "form", "iframe"]):
            tag.decompose()

        # Î•ÏƒÏ„Î¯Î±ÏƒÎ· ÏƒÏ„Î¿ ÎºÏÏÎ¹Î¿ Ï€ÎµÏÎ¹ÎµÏ‡ÏŒÎ¼ÎµÎ½Î¿: article, main, section, Î® body
        candidates = soup.find_all(['article', 'main', 'section'])
        if not candidates:
            candidates = [soup.body] if soup.body else []

        # ÎœÎ±Î¶ÎµÏÎ¿Ï…Î¼Îµ Ï„Î¿ ÏƒÎ·Î¼Î±Î½Ï„Î¹ÎºÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
        text_parts = []
        for tag in candidates:
            paragraphs = tag.find_all(['p', 'h1', 'h2', 'h3', 'li'])  # Î¼ÏŒÎ½Î¿ Î»Î¿Î³Î¹ÎºÎ¬ block Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï…
            for p in paragraphs:
                content = p.get_text(separator=' ', strip=True)
                if len(content.split()) > 5:  # Î‘Ï€ÏŒÏÏÎ¹ÏˆÎ· Ï€Î¿Î»Ï Î¼Î¹ÎºÏÏÎ½ ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½ (Ï€.Ï‡. "ok", "read more")
                    text_parts.append(content)

        clean_text = '\n'.join(text_parts)
        return clean_text if clean_text.strip() else None

    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ URL: {url}\nÎ›ÎµÏ€Ï„Î¿Î¼Î­ÏÎµÎ¹ÎµÏ‚: {e}")
        return None


def save_text_to_file(text, directory, filename):
    os.makedirs(directory, exist_ok=True)
    filepath = os.path.join(directory, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(text)
    print(f"âœ… Î‘Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ: {filepath}")

# === EXECUTION ===

base_dir = "downloaded_articles"

for label, urls in grouped_dict.items():
    print(f"\nğŸ” Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± ÎºÎ±Ï„Î·Î³Î¿ÏÎ¯Î±Ï‚: {label}")
    label_dir = os.path.join(base_dir, sanitize_filename(label))
    
    for idx, url in enumerate(urls, start=1):
        print(f"â¡ï¸ ({idx}) {url}")
        text = extract_text_from_url(url)
        if text:
            filename = f"article_{idx}.txt"
            save_text_to_file(text, label_dir, filename)

print("Finished Downloading")